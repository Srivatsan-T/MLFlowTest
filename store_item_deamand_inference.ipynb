{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SrivatsanT\\anaconda3\\envs\\StoreCasting\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get input for df\n",
    "\n",
    "df = pd.read_csv('input/test.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameGroupBy' object has no attribute 'sales'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Rolling Summary Stats Features\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m91\u001b[39m, \u001b[38;5;241m98\u001b[39m, \u001b[38;5;241m105\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m119\u001b[39m, \u001b[38;5;241m126\u001b[39m, \u001b[38;5;241m186\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m210\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m365\u001b[39m, \u001b[38;5;241m546\u001b[39m, \u001b[38;5;241m700\u001b[39m]:\n\u001b[1;32m---> 29\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_roll_mean_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msales\u001b[49m\u001b[38;5;241m.\u001b[39mrolling(i)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m#df[\"sales_roll_std_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).std().shift(1).values\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m#df[\"sales_roll_max_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).max().shift(1).values\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#df[\"sales_roll_min_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).min().shift(1).values\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Store Based\u001b[39;00m\n\u001b[0;32m     39\u001b[0m storesales \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msales\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\SrivatsanT\\anaconda3\\envs\\StoreCasting\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1363\u001b[0m, in \u001b[0;36mGroupBy.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[attr]\n\u001b[1;32m-> 1363\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1365\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrameGroupBy' object has no attribute 'sales'"
     ]
    }
   ],
   "source": [
    "# 1. Time Related Features\n",
    "#####################################################\n",
    "def create_date_features(df):\n",
    "    df['month'] = df.date.dt.month\n",
    "    df['day_of_month'] = df.date.dt.day\n",
    "    df['day_of_year'] = df.date.dt.dayofyear\n",
    "    df['week_of_year'] = df.date.dt.isocalendar().week\n",
    "    df['day_of_week'] = df.date.dt.dayofweek + 1\n",
    "    df['year'] = df.date.dt.year\n",
    "    df[\"is_wknd\"] = df.date.dt.weekday // 4\n",
    "    df[\"quarter\"] = df.date.dt.quarter\n",
    "    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df.date.dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df.date.dt.is_year_end.astype(int)\n",
    "    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n",
    "    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n",
    "    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n",
    "    df[\"season\"] = np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])\n",
    "    return df\n",
    "df = create_date_features(df)\n",
    "\n",
    "\n",
    "# Rolling Summary Stats Features\n",
    "#####################################################\n",
    "for i in [91, 98, 105, 112, 119, 126, 186, 200, 210, 250, 300, 365, 546, 700]:\n",
    "    df[\"sales_roll_mean_\"+str(i)]=df.groupby([\"store\", \"item\"]).sales.rolling(i).mean().shift(1).values\n",
    "    #df[\"sales_roll_std_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).std().shift(1).values\n",
    "    #df[\"sales_roll_max_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).max().shift(1).values\n",
    "    #df[\"sales_roll_min_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).min().shift(1).values\n",
    "\n",
    "\n",
    "# 2. Hypothesis Testing: Similarity\n",
    "#####################################################\n",
    "\n",
    "# Store Based\n",
    "storesales = train.groupby([\"date\", \"store\"]).sales.sum().reset_index()\n",
    "ctg_ss = CompareTwoGroups(storesales, group=\"store\", target=\"sales\")\n",
    "del storesales\n",
    "\n",
    "df[\"StoreSalesSimilarity\"] = np.where(df.store.isin([3,10]), 1, 0)\n",
    "df[\"StoreSalesSimilarity\"] = np.where(df.store.isin([4,9]), 2, df[\"StoreSalesSimilarity\"])\n",
    "df[\"StoreSalesSimilarity\"] = np.where(df.store.isin([5,6]), 3, df[\"StoreSalesSimilarity\"])\n",
    "\n",
    "# Item Based\n",
    "\n",
    "itemsales = train.groupby([\"date\", \"item\"]).sales.sum().reset_index()\n",
    "ctg_is = CompareTwoGroups(itemsales, group = \"item\", target = \"sales\")\n",
    "del itemsales\n",
    "\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([1,4,27,41,47]), 1, 0)\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([2,6,7,14,31,46]), 2, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([3,42]), 3, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([8,36]), 4, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([9,43,48]), 5, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([11,12,29,33]), 6, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([13,18]), 7, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([15,28]), 8, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([16,34]), 9, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([19,21,30]), 10, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([20,26]), 11, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([22,25,38,45]), 12, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([23,37,40,44,49]), 13, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([24,35,50]), 14, df[\"ItemSalesSimilarity\"])\n",
    "df[\"ItemSalesSimilarity\"] = np.where(df.item.isin([32,39]), 15, df[\"ItemSalesSimilarity\"])\n",
    "\n",
    "# 3. Lag/Shifted Features\n",
    "#####################################################\n",
    "\n",
    "# test.groupby([\"store\", \"item\"]).date.count()\n",
    "# Test verisinde +90 gün tahmin edilmesi isteniyor bu yüzden\n",
    "# Lag featureları en az 91 olmalı!\n",
    "\n",
    "df.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n",
    "\n",
    "def lag_features(dataframe, lags, groups = [\"store\", \"item\"], target = \"sales\", prefix = ''):\n",
    "    dataframe = dataframe.copy()\n",
    "    for lag in lags:\n",
    "        dataframe[prefix + str(lag)] = dataframe.groupby(groups)[target].transform(\n",
    "            lambda x: x.shift(lag))\n",
    "    return dataframe\n",
    "\n",
    "df = lag_features(df, lags = [91, 92,93,94,95,96, 97, 98, 100, 105, 112, 119, 126, 150,\n",
    "                              182,200,220, 250, 300, 350, 355, 360,361,362,363, 364,\n",
    "                              365, 370, 375,380, 546, 600, 650, 680, 690, 700, 710, 728,\n",
    "                              730, 800, 900, 950, 990, 1000, 1050, 1090, 1095],\n",
    "                  groups = [\"store\", \"item\"], target = 'sales', prefix = 'sales_lag_')\n",
    "\n",
    "def drop_cor(dataframe, name, index):\n",
    "    ind = dataframe[dataframe.columns[dataframe.columns.str.contains(name)].tolist()+[\"sales\"]].corr().sales.sort_values(ascending = False).index[1:index]\n",
    "    ind = dataframe.drop(ind, axis = 1).columns[dataframe.drop(ind, axis = 1).columns.str.contains(name)]\n",
    "    dataframe.drop(ind, axis = 1, inplace = True)\n",
    "\n",
    "drop_cor(df, \"sales_lag\", 16)\n",
    "\n",
    "\n",
    "# 4. Last i. Months\n",
    "#####################################################\n",
    "df[\"monthyear\"] = df.date.dt.to_period('M')\n",
    "\n",
    "# Store-Item Based\n",
    "for i in [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36]:\n",
    "    last_months = df.groupby([\"store\", \"item\", \"monthyear\"]).sales.agg([\n",
    "        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n",
    "    last_months.columns = ['store', 'item', 'monthyear', 'last_'+str(i)+'months_sales_sum',\n",
    "                           'last_'+str(i)+'months_sales_mean', 'last_'+str(i)+'months_sales_std',\n",
    "                           'last_'+str(i)+'months_sales_min', 'last_'+str(i)+'months_sales_max']\n",
    "    df = pd.merge(df, last_months, how   = \"left\", on = [\"store\", \"item\", \"monthyear\"])\n",
    "del last_months, i\n",
    "\n",
    "drop_cor(df, \"last_\", 15)\n",
    "\n",
    "# Store Based\n",
    "\n",
    "\n",
    "for i in [3, 6, 9, 12]:\n",
    "    last_months = df.groupby([\"store\", \"monthyear\"]).sales.agg([\n",
    "        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n",
    "    last_months.columns = ['store', 'monthyear', 'store_last_'+str(i)+'months_sales_sum',\n",
    "                           'store_last_'+str(i)+'months_sales_mean', 'store_last_'+str(i)+'months_sales_std',\n",
    "                           'store_last_'+str(i)+'months_sales_min', 'store_last_'+str(i)+'months_sales_max']\n",
    "    df = pd.merge(df, last_months, how = \"left\", on = [\"store\", \"monthyear\"])\n",
    "del last_months, i\n",
    "\n",
    "# Item Based\n",
    "for i in [3, 6, 9, 12]:\n",
    "    last_months = df.groupby([\"item\", \"monthyear\"]).sales.agg([\n",
    "        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n",
    "    last_months.columns = ['item', 'monthyear', 'item_last_'+str(i)+'months_sales_sum',\n",
    "                           'item_last_'+str(i)+'months_sales_mean', 'item_last_'+str(i)+'months_sales_std',\n",
    "                           'item_last_'+str(i)+'months_sales_min', 'item_last_'+str(i)+'months_sales_max']\n",
    "    df = pd.merge(df, last_months, how = \"left\", on = [\"item\", \"monthyear\"])\n",
    "del last_months, i\n",
    "\n",
    "# Similarity Based\n",
    "\n",
    "\n",
    "for i in [3, 6, 9, 12]:\n",
    "    last_months = df.groupby([\"StoreSalesSimilarity\", \"monthyear\"]).sales.agg([\n",
    "        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n",
    "    last_months.columns = ['StoreSalesSimilarity', 'monthyear', 'storesim_last_'+str(i)+'months_sales_sum',\n",
    "                           'storesim_last_'+str(i)+'months_sales_mean', 'storesim_last_'+str(i)+'months_sales_std',\n",
    "                           'storesim_last_'+str(i)+'months_sales_min', 'storesim_last_'+str(i)+'months_sales_max']\n",
    "    df = pd.merge(df, last_months, how = \"left\", on = [\"StoreSalesSimilarity\", \"monthyear\"])\n",
    "del last_months, i\n",
    "\n",
    "\n",
    "for i in [3, 6, 9, 12]:\n",
    "    last_months = df.groupby([\"ItemSalesSimilarity\", \"monthyear\"]).sales.agg([\n",
    "        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n",
    "    last_months.columns = ['ItemSalesSimilarity', 'monthyear', 'itemsim_last_'+str(i)+'months_sales_sum',\n",
    "                           'itemsim_last_'+str(i)+'months_sales_mean', 'itemsim_last_'+str(i)+'months_sales_std',\n",
    "                           'itemsim_last_'+str(i)+'months_sales_min', 'itemsim_last_'+str(i)+'months_sales_max']\n",
    "    df = pd.merge(df, last_months, how = \"left\", on = [\"ItemSalesSimilarity\", \"monthyear\"])\n",
    "del last_months, i\n",
    "\n",
    "df.drop(\"monthyear\", axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# 5. Last i. day of week\n",
    "#####################################################\n",
    "df.sort_values([\"store\", \"item\", \"day_of_week\", \"date\"], inplace = True)\n",
    "\n",
    "df = lag_features(df, lags = np.arange(12,41, 1).tolist()+[91, 92, 95, 98, 99, 100, 105, 112, 119, 126, 133, 140, 200, 205, 210, 215, 220, 250],\n",
    "                  groups = [\"store\", \"item\", \"day_of_week\"], target = 'sales', prefix = 'dayofweek_sales_lag_')\n",
    "\n",
    "df[df.columns[df.columns.str.contains(\"dayofweek_sales_lag_\")].tolist()+[\"sales\"]].corr().sales.sort_values(ascending = False)\n",
    "\n",
    "drop_cor(df, \"dayofweek_sales_lag_\", 16)\n",
    "\n",
    "df.sort_values([\"store\", \"item\", \"date\"], inplace = True)\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# Exponentially Weighted Mean Features\n",
    "#####################################################\n",
    "def ewm_features(dataframe, alphas, lags):\n",
    "    dataframe = dataframe.copy()\n",
    "    for alpha in alphas:\n",
    "        for lag in lags:\n",
    "            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n",
    "                dataframe.groupby([\"store\", \"item\"])['sales']. \\\n",
    "                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "    return dataframe\n",
    "\n",
    "alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n",
    "lags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n",
    "\n",
    "df = ewm_features(df, alphas, lags)\n",
    "\n",
    "# Day of year \n",
    "df.sort_values([\"day_of_year\", \"store\", \"item\"], inplace = True)\n",
    "df = lag_features(df, lags = [1,2,3,4],\n",
    "                  groups = [\"day_of_year\", \"store\", \"item\"], target = 'sales', prefix = 'dayofyear_sales_lag_')\n",
    "\n",
    "\n",
    "# pd.cut\n",
    "clus = df.groupby([\"store\"]).sales.mean().reset_index()\n",
    "clus[\"store_cluster\"] =  pd.cut(clus.sales, bins = 4, labels = range(1,5))\n",
    "clus.drop(\"sales\", axis = 1, inplace = True)\n",
    "df = pd.merge(df, clus, how = \"left\")\n",
    "clus = df.groupby([\"item\"]).sales.mean().reset_index()\n",
    "clus[\"item_cluster\"] =  pd.cut(clus.sales, bins = 5, labels = range(1,6))\n",
    "clus.drop(\"sales\", axis = 1, inplace = True)\n",
    "df = pd.merge(df, clus, how = \"left\")\n",
    "del clus\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only Important Features\n",
    "\n",
    "imp_features = ['sales_lag_364', 'sales_lag_350', 'dayofweek_sales_lag_12', 'item_cluster', 'last_12months_sales_mean', 'sales_ewm_alpha_05_lag_365', 'day_of_week', 'dayofweek_sales_lag_13', 'season', 'sales_ewm_alpha_09_lag_91', 'store_cluster', 'sales_ewm_alpha_08_lag_91', 'last_12months_sales_sum', 'sales_ewm_alpha_07_lag_728', 'dayofweek_sales_lag_105', 'sales_ewm_alpha_05_lag_728', 'sales_ewm_alpha_095_lag_98', 'sales_ewm_alpha_07_lag_365', 'storesim_last_6months_sales_mean', 'month', 'sales_ewm_alpha_095_lag_91', 'last_3months_sales_mean', 'sales_ewm_alpha_07_lag_270', 'sales_ewm_alpha_05_lag_270', 'sales_ewm_alpha_07_lag_91', 'sales_ewm_alpha_095_lag_270', 'sales_lag_363', 'sales_ewm_alpha_08_lag_728', 'sales_ewm_alpha_095_lag_105', 'week_of_year', 'storesim_last_6months_sales_min', 'itemsim_last_9months_sales_std', 'dayofweek_sales_lag_40', 'is_wknd', 'day_of_year', 'sales_lag_700', 'itemsim_last_12months_sales_std', 'sales_lag_370', 'sales_ewm_alpha_08_lag_105', 'sales_ewm_alpha_08_lag_98', 'store_last_6months_sales_mean', 'itemsim_last_12months_sales_min', 'sales_ewm_alpha_09_lag_728', 'sales_ewm_alpha_07_lag_98', 'store_last_6months_sales_sum', 'sales_ewm_alpha_09_lag_98', 'dayofyear_sales_lag_2', 'sales_ewm_alpha_08_lag_112', 'sales_ewm_alpha_05_lag_105', 'last_3months_sales_sum', 'sales_lag_362', 'item_last_6months_sales_min', 'store_last_12months_sales_mean', 'sales_ewm_alpha_095_lag_112', 'itemsim_last_12months_sales_mean', 'ItemSalesSimilarity', 'sales_ewm_alpha_095_lag_728', 'item_last_6months_sales_std', 'sales_ewm_alpha_08_lag_365', 'dayofweek_sales_lag_112', 'itemsim_last_6months_sales_min', 'sales_ewm_alpha_07_lag_112', 'sales_ewm_alpha_07_lag_105', 'dayofyear_sales_lag_4', 'last_9months_sales_mean', 'storesim_last_12months_sales_sum']\n",
    "df = df[imp_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Best params from already performed experiments\n",
    "#Selecting the model from the pickle file\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Load a pickled model from a file\n",
    "with open('StoreCasting.pkl', 'rb') as file:\n",
    "    storecasting_model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pd.DataFrame({\n",
    "    \"date\":df.date,\n",
    "    \"store\":df.store,\n",
    "    \"item\":df.item,\n",
    "    \"sales\":storecasting_model.predict(df)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = 1\n",
    "sub = train[train.store == store].set_index(\"date\")\n",
    "forc = forecast[forecast.store == store].set_index(\"date\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(10, 5, figsize=(20, 35))\n",
    "for i in range(1,51):\n",
    "    if i < 6:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n",
    "        forc[forc.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Forecast\")\n",
    "    if i >= 6 and i<11:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n",
    "        forc[forc.item == i].sales.plot(ax=axes[1, i-6], legend=True, label = \"Forecast\")\n",
    "    if i >= 11 and i<16:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\") \n",
    "        forc[forc.item == i].sales.plot(ax=axes[2, i-11], legend=True, label = \"Forecast\")\n",
    "    if i >= 16 and i<21:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n",
    "        forc[forc.item == i].sales.plot(ax=axes[3, i-16], legend=True, label = \"Forecast\")\n",
    "    if i >= 21 and i<26:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\") \n",
    "        forc[forc.item == i].sales.plot(ax=axes[4, i-21], legend=True, label = \"Forecast\")\n",
    "    if i >= 26 and i<31:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")\n",
    "        forc[forc.item == i].sales.plot(ax=axes[5, i-26], legend=True, label = \"Forecast\")\n",
    "    if i >= 31 and i<36:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")  \n",
    "        forc[forc.item == i].sales.plot(ax=axes[6, i-31], legend=True, label = \"Forecast\")\n",
    "    if i >= 36 and i<41:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")\n",
    "        forc[forc.item == i].sales.plot(ax=axes[7, i-36], legend=True, label = \"Forecast\")\n",
    "    if i >= 41 and i<46:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\") \n",
    "        forc[forc.item == i].sales.plot(ax=axes[8, i-41], legend=True, label = \"Forecast\")\n",
    "    if i >= 46 and i<51:\n",
    "        sub[sub.item == i].sales.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\") \n",
    "        forc[forc.item == i].sales.plot(ax=axes[9, i-46], legend=True, label = \"Forecast\")\n",
    "plt.tight_layout(pad=6.5)\n",
    "plt.suptitle(\"Store 1 Items Actual & Forecast\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StoreCasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
